{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_projet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RMIK4foQxLA",
        "colab_type": "code",
        "outputId": "c54d4eec-b78b-4b05-ab18-f9400e99bad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from math import sqrt\n",
        "from math import exp\n",
        "\n",
        "import gensim\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn import svm\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "\n",
        "\n",
        "def pre_processing(reviews):\n",
        "    reviews.index = range(len(reviews))\n",
        "    reviews = lower_case(reviews, \"reviews.text\")\n",
        "    reviews = punctuation_tokenizer(data=reviews, col='reviews.text')\n",
        "    reviews = retire_small_words(data=reviews, col='reviews.tokenize', treshold=3)\n",
        "    reviews = retire_stop_words(data=reviews, col_to_reduce='reviews.tokenize')\n",
        "    reviews = retire_numbers(data=reviews, col='reviews.tokenize')\n",
        "    reviews = stem_reviews(df=reviews, col='reviews.tokenize')\n",
        "    reviews = lemmatize(data=reviews, col_to_lemmatize='reviews.tokenize')\n",
        "    return reviews\n",
        "\n",
        "\n",
        "# Pre Processing\n",
        "def bag_of_word(df, col, dict_of_words):\n",
        "    bag_of_words_df = pd.DataFrame(index=range(len(df)), columns=list(dict_of_words.keys()))\n",
        "    bag_of_words_df.fillna(0, inplace=True)\n",
        "    bag_of_words_df['sentence_length'] = 0\n",
        "\n",
        "    index_current_sentence = 0\n",
        "\n",
        "    for sentence in df[col]:\n",
        "        bag_of_words_df.at[index_current_sentence, 'sentence_length'] = len(sentence)\n",
        "        for word in sentence:\n",
        "            # si le mot n'est pas dans le df --> mot pas important alors on va dans le except\n",
        "            try:\n",
        "                if bag_of_words_df.at[index_current_sentence, word] == 0:\n",
        "                    bag_of_words_df.at[index_current_sentence, word] = 1\n",
        "                else:\n",
        "                    bag_of_words_df.at[index_current_sentence, word] = \\\n",
        "                        bag_of_words_df.at[index_current_sentence, word] + 1\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        index_current_sentence = index_current_sentence + 1\n",
        "\n",
        "    return bag_of_words_df\n",
        "\n",
        "\n",
        "def stem_reviews(df, col):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemme_list = []\n",
        "    for sentence in df[col]:\n",
        "        stemme_list.append([stemmer.stem(word) for word in sentence])\n",
        "\n",
        "    df[col] = stemme_list\n",
        "    return df\n",
        "\n",
        "\n",
        "def retire_numbers(data, col):\n",
        "    index_current_sentence = 0\n",
        "    for sentence in data[col]:\n",
        "        index_current_word = 0\n",
        "        for word in sentence:\n",
        "            if bool(re.findall(r'[0-9]+', string=word)):\n",
        "                sentence.remove(word)\n",
        "\n",
        "            index_current_word = index_current_word + 1\n",
        "\n",
        "        index_current_sentence = index_current_sentence + 1\n",
        "    return data\n",
        "\n",
        "\n",
        "# lower_case\n",
        "def lower_case(data, col):\n",
        "    data[col] = data[col].apply(lambda x: x.lower())\n",
        "    return data\n",
        "\n",
        "\n",
        "def punctuation_tokenizer(data, col):\n",
        "    punctuation_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    data['reviews.tokenize'] = data[col].apply(lambda review: punctuation_tokenizer.tokenize(review))\n",
        "    return data\n",
        "\n",
        "\n",
        "def retire_stop_words(data, col_to_reduce):\n",
        "    reduced_reviews = []\n",
        "    stops = set(stopwords.words('english'))\n",
        "\n",
        "    for review in data[col_to_reduce]:\n",
        "        reduced_reviews.append(list(word for word in review if word not in stops))\n",
        "\n",
        "    data[col_to_reduce] = reduced_reviews\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def retire_small_words(data, col, treshold):\n",
        "    reduced_reviews = []\n",
        "\n",
        "    for review in data[col]:\n",
        "        reduced_reviews.append(list(word for word in review if len(word) > treshold))\n",
        "\n",
        "    data[col] = reduced_reviews\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# Lemmatize\n",
        "def lemmatize_list(lemmatizer, list_of_word):\n",
        "    lemmatized_words = []\n",
        "    for word in list_of_word:\n",
        "        lemmatized_words.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "    return lemmatized_words\n",
        "\n",
        "\n",
        "def lemmatize(data, col_to_lemmatize):\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    lemmatize_review = []\n",
        "\n",
        "    for review in data[col_to_lemmatize]:\n",
        "        lemmatize_review.append(lemmatize_list(lemmatizer=lemmatizer, list_of_word=review))\n",
        "\n",
        "    data['reviews.tokenize'] = lemmatize_review\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_word2vec_model():\n",
        "    # model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True,limit=500000)\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec', binary=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vectorize_sentence(vectorization_model, reviews, col_to_vectorize, dict_tfidf):\n",
        "    data = pd.DataFrame(index=range(len(reviews)), dtype=np.float)\n",
        "    word_tresh_hold = 500\n",
        "\n",
        "    for word in range(word_tresh_hold):\n",
        "        data[f'word_vector_{word}'] = 0\n",
        "\n",
        "    index_current_sentance = 0\n",
        "    for sentence in reviews[col_to_vectorize]:\n",
        "        index_current_word = 0\n",
        "        for word in sentence:\n",
        "            if index_current_word < word_tresh_hold:\n",
        "                # todo pca on sentence in stead of word2vec on word\n",
        "                try:\n",
        "                    data.at[index_current_sentance, f'word_vector_{index_current_word}'] = \\\n",
        "                        vectorization_model[word].mean() * 100000000 * dict_tfidf[word]\n",
        "                    index_current_word = index_current_word + 1\n",
        "                except:\n",
        "                    index_current_word = index_current_word + 1\n",
        "                    continue\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        index_current_sentance = index_current_sentance + 1\n",
        "\n",
        "    normalizer = Normalizer().fit(data)\n",
        "    normalized_data = normalizer.transform(data)\n",
        "\n",
        "    normalized_df = pd.DataFrame(normalized_data, index=data.index, columns=data.columns)\n",
        "\n",
        "    return normalized_df\n",
        "\n",
        "\n",
        "def reduced_word_vectors_with_PCA(data, n_components):\n",
        "    pca = PCA(n_components=n_components)\n",
        "    comments = list(data.iterrows())\n",
        "\n",
        "    reduced_df = pd.DataFrame(index=data.index)\n",
        "    for i in range(len(comments)):\n",
        "        reduced_df[i] = pca.fit_transform(comments[i])\n",
        "\n",
        "    # pca.fit(list_of_word_vectors)\n",
        "    return pca.singular_values_\n",
        "\n",
        "\n",
        "def reduced_vocab_based_on_tfidf(dict_tf_idf, number_of_words_to_keep):\n",
        "    tfidf_df = pd.DataFrame(dict_tf_idf.items(), index=range(len(dict_tf_idf)))\n",
        "    tfidf_df.sort_values([1], axis=0, inplace=True, ascending=False)\n",
        "\n",
        "    # keep only n words with best tfidf\n",
        "    tfidf_df = tfidf_df[:number_of_words_to_keep]\n",
        "\n",
        "    reduced_dict_tf_idf = {}\n",
        "\n",
        "    for word in dict_tf_idf.keys():\n",
        "        if word in list(tfidf_df[0]):\n",
        "            reduced_dict_tf_idf[word] = dict_tf_idf[word]\n",
        "\n",
        "    return reduced_dict_tf_idf\n",
        "\n",
        "\n",
        "def filter_vocab_based_on_tf_idf(dict_tf_idf, df, col, tf_idf_treshold):\n",
        "    for sentence in df[col]:\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                if dict_tf_idf[word] <= tf_idf_treshold:\n",
        "                    sentence.remove(word)\n",
        "            except:\n",
        "                continue\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_tfidf(df, col, tokenized_text=False):\n",
        "    corpus = []\n",
        "    list_of_word = []\n",
        "    if not tokenized_text:\n",
        "        df = punctuation_tokenizer(data=df, col=col)\n",
        "\n",
        "    for sentence in df['reviews.tokenize']:\n",
        "        joined_sentence = \"\"\n",
        "        for word in sentence:\n",
        "            # filter number\n",
        "            if not bool(re.findall(r'[0-9]+', string=word)):\n",
        "                joined_sentence = joined_sentence + \" \" + word\n",
        "                list_of_word.append(word)\n",
        "\n",
        "        corpus.append(joined_sentence)\n",
        "\n",
        "    tfidf_vector = TfidfVectorizer()\n",
        "    tfidf = tfidf_vector.fit_transform(corpus)\n",
        "\n",
        "    tfidf_dict = dict(zip(tfidf_vector.get_feature_names(), tfidf.data))\n",
        "\n",
        "    return tfidf_dict, corpus\n",
        "\n",
        "\n",
        "def RMSE(prediction, target):\n",
        "    rms = sqrt(mean_squared_error(target, prediction))\n",
        "    return rms\n",
        "\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "\n",
        "def dict_words(reviews):\n",
        "    list_word = {}\n",
        "    for comment in reviews['reviews.tokenize']:\n",
        "        for word in comment:\n",
        "            list_word[word] = 0\n",
        "\n",
        "    return list_word\n",
        "\n",
        "\n",
        "def sentiWord(dict_word):\n",
        "    reduced_dict_with_sent = {}\n",
        "\n",
        "    for word in dict_word.keys():\n",
        "        result = list(swn.senti_synsets(word))\n",
        "        if result:\n",
        "            reduced_dict_with_sent[word] = {f'pos_score': result[0].pos_score(),\n",
        "                                            f'neg_score': result[0].neg_score(),\n",
        "                                            f'obj_score': result[0].obj_score()}\n",
        "        # else:\n",
        "        #     dict_word[word] = {f'pos_score': 0,\n",
        "        #                        f'neg_score': 0,\n",
        "        #                        f'obj_score': 0}\n",
        "\n",
        "    return reduced_dict_with_sent\n",
        "\n",
        "\n",
        "def type_of_word(df):\n",
        "    list_type = []\n",
        "    res = nltk.pos_tag(df['Words'])\n",
        "    for word in res:\n",
        "        list_type.append(word[1])\n",
        "\n",
        "    df['Type'] = list_type\n",
        "\n",
        "\n",
        "def get_df_with_sentim_analysis(senti_dict, df, word2vec_model=None):\n",
        "    if not word2vec_model:\n",
        "        for word in df.columns:\n",
        "            try:\n",
        "                if senti_dict[word]['pos_score'] >= senti_dict[word]['neg_score']:\n",
        "                    # on cherche dans le dataframe quand le mot est prÃ©sent dans un commentaire\n",
        "                    for index in df[df[word] != 0].index:\n",
        "                        df.at[index, word] = df.at[index, word] + senti_dict[word]['pos_score'] + senti_dict[word]['obj_score']\n",
        "                else:\n",
        "                    for index in df[df[word] != 0].index:\n",
        "                        df.at[index, word] = - (df.at[index, word] + senti_dict[word]['neg_score'] + senti_dict[word]['obj_score'])\n",
        "            except:\n",
        "                continue\n",
        "    else:\n",
        "        for word in df.columns:\n",
        "            try:\n",
        "                if senti_dict[word]['pos_score'] >= senti_dict[word]['neg_score']:\n",
        "                    df[word] = df[word] + senti_dict[word]['pos_score'] + word2vec_model[word].mean()\n",
        "                else:\n",
        "                    df[word] = - df[word] + senti_dict[word]['neg_score'] + word2vec_model[word].mean()\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def vectorize_reviews(word2vec_model, df, col):\n",
        "    punctuation_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    df['reviews.to_vectorize'] = df[col].apply(lambda review: punctuation_tokenizer.tokenize(review))\n",
        "    reviews_vectors = []\n",
        "    pca = PCA(n_components=1)\n",
        "\n",
        "    for sentence in df['reviews.to_vectorize']:\n",
        "        vectorized_words = []\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                vectorized_words.append(word2vec_model[word])\n",
        "            except:\n",
        "                continue\n",
        "        if vectorized_words:\n",
        "            try:\n",
        "                pca_result = pca.fit_transform(vectorized_words)\n",
        "            except:\n",
        "                pca_result = np.array([0] * len(sentence))\n",
        "\n",
        "            reviews_vectors.append(sum(pca_result)[0])\n",
        "        else:\n",
        "            reviews_vectors.append(0)\n",
        "\n",
        "    return reviews_vectors\n",
        "\n",
        "\n",
        "def Normalize(df):\n",
        "    normalizer = Normalizer().fit(df)\n",
        "    normalized_data = normalizer.transform(df)\n",
        "    df = pd.DataFrame(\n",
        "        normalized_data,\n",
        "        index=df.index,\n",
        "        columns=df.columns)\n",
        "    return df\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvkO9VgHQ4nW",
        "colab_type": "code",
        "outputId": "5ad36bd2-beae-4a64-a810-3868d3084ee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authentification Google\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "# Download du fichier\n",
        "id = '1jX82w9vyYjtqz_8p_6N5SnEVhI4kyQbR'\n",
        "downloaded = drive.CreateFile({'id': id}) \n",
        "downloaded.GetContentFile('hotel_reviews.csv')\n",
        "\n",
        "id_model = '1dbvPvUpzCPkN3kUXwm22KUsawvOxHRzq'\n",
        "downloaded_model = drive.CreateFile({'id': id_model}) \n",
        "downloaded_model.GetContentFile('wiki-news-300d-1M.vec')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    reviews = pd.read_csv('http://christophe-rodrigues.fr/hotel_reviews.csv' ,sep=';',encoding='utf-8')\n",
        "    reviews.drop(\"Unnamed: 0\", inplace=True, axis=1)\n",
        "\n",
        "    print(\"Split Train and Test\")\n",
        "    train_set, test_set = train_test_split(reviews, test_size=0.20, shuffle=True)\n",
        "    train_set.index = range(len(train_set))\n",
        "    test_set.index = range(len(test_set))\n",
        "\n",
        "    print(\"Pre processing\")\n",
        "    pre_processed_trained_reviews = pre_processing(reviews=train_set)\n",
        "    pre_processed_test_reviews = pre_processing(reviews=test_set)\n",
        "\n",
        "    dict_tfidf_trained, corpus = get_tfidf(pre_processed_trained_reviews, 'reviews.tokenize', tokenized_text=True)\n",
        "    dict_tfidf_test, _ = get_tfidf(pre_processed_test_reviews, 'reviews.tokenize', tokenized_text=True)\n",
        "\n",
        "    dict_words_trained = reduced_vocab_based_on_tfidf(\n",
        "        dict_tf_idf=dict_tfidf_trained,\n",
        "        number_of_words_to_keep=1400)\n",
        "\n",
        "    dict_words_test = reduced_vocab_based_on_tfidf(\n",
        "        dict_tf_idf=dict_tfidf_test,\n",
        "        number_of_words_to_keep=1400)\n",
        "\n",
        "    pos_neg_trained_word_dict = sentiWord(dict_words_trained)\n",
        "    pos_neg_test_word_dict = sentiWord(dict_words_test)\n",
        "\n",
        "    train_bag_of_word_df = bag_of_word(\n",
        "        df=pre_processed_trained_reviews,\n",
        "        col=\"reviews.tokenize\",\n",
        "        dict_of_words=pos_neg_trained_word_dict)\n",
        "\n",
        "    test_bag_of_word_df = bag_of_word(\n",
        "        df=pre_processed_test_reviews,\n",
        "        col=\"reviews.tokenize\",\n",
        "        dict_of_words=pos_neg_trained_word_dict)\n",
        "\n",
        "    print(\"load Word2Vec model\")\n",
        "    model = load_word2vec_model()\n",
        "\n",
        "    print('Vectorizing')\n",
        "    trained_vectors = vectorize_reviews(df=pre_processed_trained_reviews, col='reviews.text', word2vec_model=model)\n",
        "    test_vectors = vectorize_reviews(df=pre_processed_test_reviews, col='reviews.text', word2vec_model=model)\n",
        "\n",
        "    print('Getting sentiment analysis')\n",
        "    senti_analysis_trained_df = get_df_with_sentim_analysis(\n",
        "        senti_dict=pos_neg_trained_word_dict,\n",
        "        df=train_bag_of_word_df)\n",
        "    senti_analysis_test_df = get_df_with_sentim_analysis(\n",
        "        senti_dict=pos_neg_test_word_dict,\n",
        "        df=test_bag_of_word_df)\n",
        "\n",
        "    senti_analysis_trained_df['vectors'] = trained_vectors\n",
        "    senti_analysis_test_df['vectors'] = test_vectors\n",
        "\n",
        "    senti_analysis_trained_df = Normalize(df=senti_analysis_trained_df)\n",
        "    senti_analysis_test_df = Normalize(df=senti_analysis_test_df)\n",
        "\n",
        "    print('Training')\n",
        "    reg = svm.SVR(C=20,epsilon=0.1)\n",
        "    reg.fit(senti_analysis_trained_df, train_set['reviews.rating'])\n",
        "    print('Predicting')\n",
        "    prediction = reg.predict(senti_analysis_test_df)\n",
        "    target = test_set[\"reviews.rating\"]\n",
        "\n",
        "    print(f'RMSE : {RMSE(prediction, target)}')\n",
        "    print(f'MAPE : {mean_absolute_percentage_error(y_pred=prediction, y_true=target)}')\n",
        "    print(f'Precision : {100 - mean_absolute_percentage_error(y_pred=prediction, y_true=target)}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Split Train and Test\n",
            "Pre processing\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:97: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:103: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:125: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:114: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:77: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:149: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "load Word2Vec model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Vectorizing\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:328: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:328: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting sentiment analysis\n",
            "Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicting\n",
            "RMSE : 1.136058161207676\n",
            "MAPE : 36.437041376920284\n",
            "Precision : 63.562958623079716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDivaZJN4f-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to process data to be able to train or predict with model\n",
        "def data_prep(df):\n",
        "  print(\"Pre processing\")\n",
        "  pre_processed_reviews = pre_processing(reviews=df)\n",
        "\n",
        "  # dict_tfidf, corpus = get_tfidf(pre_processed_reviews, 'reviews.tokenize', tokenized_text=True)\n",
        "\n",
        "  # dict_tfidf = reduced_vocab_based_on_tfidf(\n",
        "  #     dict_tf_idf=dict_tfidf,\n",
        "  #     number_of_words_to_keep=1400)\n",
        "\n",
        "  # pos_neg_trained_word_dict = sentiWord(dict_tfidf)\n",
        "\n",
        "  bag_of_word_df = bag_of_word(\n",
        "      df=pre_processed_reviews,\n",
        "      col=\"reviews.tokenize\",\n",
        "      dict_of_words=pos_neg_trained_word_dict)\n",
        "\n",
        "  print('Vectorizing')\n",
        "  review_vectors = vectorize_reviews(df=pre_processed_reviews, col='reviews.text', word2vec_model=model)\n",
        "\n",
        "  print('Getting sentiment analysis')\n",
        "  senti_analysis_df = get_df_with_sentim_analysis(\n",
        "      senti_dict=pos_neg_trained_word_dict,\n",
        "      df=bag_of_word_df)\n",
        "\n",
        "  senti_analysis_df['vectors'] = review_vectors\n",
        "\n",
        "  senti_analysis_df = Normalize(df=senti_analysis_df)\n",
        "\n",
        "  return senti_analysis_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxo_hWvWlYE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import random\n",
        "\n",
        "#loading eval datas\n",
        "column_names = ['reviews.rating','reviews.text']\n",
        "eval_data = pd.read_csv('http://christophe-rodrigues.fr/eval_reviews.csv', usecols=column_names, sep=\";\")\n",
        "\n",
        "#here an example of the simplest possible model\n",
        "#take reviews in input and return ratings \n",
        "def my_random_model(reviews):\n",
        "  res = []\n",
        "  for review in reviews:\n",
        "      res.append(1+4*random.random()) #any real between [1;5]\n",
        "  return pd.DataFrame(res)\n",
        "\n",
        "#you need to adapt this line replacing this random model by yours best model.\n",
        "#take in input all the text reviews (not ratings) and returns predicted ratings.\n",
        "pre_proccessed_data = data_prep(eval_data)\n",
        "eval_predicted = reg.predict(pre_proccessed_data)\n",
        "\n",
        "for i in range(len(eval_predicted)):\n",
        "  eval_predicted[i] = round(eval_predicted[i])\n",
        "\n",
        "print(mean_squared_error(eval_predicted,eval_data['reviews.rating']))\n",
        "#your MSE must be lower that this one, otherwise your are worst than random :-p\n",
        "#In case your model is a regresssion one, don't forget to round ratings before calling MSE. (It will allow me to compare classification and regression models fairly)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoN7jEoc3gGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}